{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 20:58:14.390672: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-12-23 20:58:14.390743: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from forward import solve_forward\n",
    "from simulation import Simulation\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "# to load large .mat files\n",
    "import mat73\n",
    "import random\n",
    "from net import EEG_CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_topos = mat73.loadmat('/media/thanos/Elements/thanos/sim_data/sim_type_1/eeg_topos_2TeD.mat')['eeg_topos']\n",
    "eeg_topos = eeg_topos.transpose(2, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Topography for eeg signal: 4940')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "topo_idx = random.randint(0,eeg_topos.shape[0]-1)\n",
    "topo = eeg_topos[topo_idx,:,:]\n",
    "plt.contourf(topo, cmap=cm.get_cmap('viridis'))\n",
    "cbar = plt.colorbar()\n",
    "plt.draw()\n",
    "plt.title('Topography for eeg signal: {}'.format(topo_idx+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward problem is solved.\n",
      "Electrodes: (73, 3)\n",
      "Dipoles: (50460, 3)\n",
      "Leadfield: (73, 50460)\n"
     ]
    }
   ],
   "source": [
    "# load previous simulation\n",
    "sources = np.load('/media/thanos/Elements/thanos/sim_data/sim_type_1/sources_2TeD.npy')\n",
    "eeg = np.load('/media/thanos/Elements/thanos/sim_data/sim_type_1/eeg_2TeD.npy')\n",
    "\n",
    "fwd = solve_forward()\n",
    "sim = Simulation(fwd=fwd, source_data=sources, eeg_data=eeg)\n",
    "# fwd = solve_forward()\n",
    "# sim = Simulation(fwd=fwd)\n",
    "# sim.simulate(n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegcnn = EEG_CNN(sim=sim, eeg_topographies=eeg_topos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-12-23 21:08:35.405011: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2021-12-23 21:08:35.407111: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-12-23 21:08:35.408774: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (thanos): /proc/driver/nvidia/version does not exist\n",
      "2021-12-23 21:08:35.428113: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-12-23 21:08:36.282249: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 276889600 exceeds 10% of free system memory.\n",
      "2021-12-23 21:08:37.133449: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 276889600 exceeds 10% of free system memory.\n",
      "2021-12-23 21:08:37.986123: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 276889600 exceeds 10% of free system memory.\n",
      "2021-12-23 21:08:39.538122: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 413368320 exceeds 10% of free system memory.\n",
      "2021-12-23 21:08:39.716094: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 413368320 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model in EEG_CNN :\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 65, 65, 32)        320       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 135200)            0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               69222912  \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 512)              2048      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 1024)             4096      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2048)              2099200   \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 2048)             8192      \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " OutputLayer (Dense)         (None, 50460)             103392540 \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 175,254,620\n",
      "Trainable params: 175,247,452\n",
      "Non-trainable params: 7,168\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# eegcnn.build_model()\n",
    "eegcnn.load_nn('/media/thanos/Elements/thanos/nn_trained/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/380\n",
      "631/631 [==============================] - 1537s 2s/step - loss: 4.5098e-05 - val_loss: 4.1810e-05\n",
      "Epoch 2/380\n",
      "631/631 [==============================] - 1432s 2s/step - loss: 4.7800e-05 - val_loss: 7.6407e-05\n",
      "Epoch 3/380\n",
      "631/631 [==============================] - 1449s 2s/step - loss: 4.9925e-05 - val_loss: 4.6169e-05\n",
      "Epoch 4/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 5.0331e-05 - val_loss: 5.6704e-05\n",
      "Epoch 5/380\n",
      "631/631 [==============================] - 1418s 2s/step - loss: 5.0950e-05 - val_loss: 6.8267e-05\n",
      "Epoch 6/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 5.1360e-05 - val_loss: 4.8518e-05\n",
      "Epoch 7/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 4.8916e-05 - val_loss: 7.3926e-05\n",
      "Epoch 8/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 4.9374e-05 - val_loss: 6.6789e-05\n",
      "Epoch 9/380\n",
      "631/631 [==============================] - 1407s 2s/step - loss: 4.9334e-05 - val_loss: 5.2197e-05\n",
      "Epoch 10/380\n",
      "631/631 [==============================] - 1404s 2s/step - loss: 4.9051e-05 - val_loss: 7.8886e-05\n",
      "Epoch 11/380\n",
      "631/631 [==============================] - 1393s 2s/step - loss: 4.8708e-05 - val_loss: 1.0447e-04\n",
      "Epoch 12/380\n",
      "631/631 [==============================] - 1389s 2s/step - loss: 4.8469e-05 - val_loss: 5.1170e-05\n",
      "Epoch 13/380\n",
      "631/631 [==============================] - 1384s 2s/step - loss: 4.7359e-05 - val_loss: 5.6039e-05\n",
      "Epoch 14/380\n",
      "631/631 [==============================] - 1380s 2s/step - loss: 4.8097e-05 - val_loss: 4.8670e-05\n",
      "Epoch 15/380\n",
      "631/631 [==============================] - 1378s 2s/step - loss: 4.7985e-05 - val_loss: 6.1658e-05\n",
      "Epoch 16/380\n",
      "631/631 [==============================] - 1374s 2s/step - loss: 4.7137e-05 - val_loss: 4.9773e-05\n",
      "Epoch 17/380\n",
      "631/631 [==============================] - 1376s 2s/step - loss: 4.7191e-05 - val_loss: 8.7864e-05\n",
      "Epoch 18/380\n",
      "631/631 [==============================] - 1374s 2s/step - loss: 4.6458e-05 - val_loss: 7.8536e-05\n",
      "Epoch 19/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 4.6893e-05 - val_loss: 5.8906e-05\n",
      "Epoch 20/380\n",
      "631/631 [==============================] - 1370s 2s/step - loss: 4.6563e-05 - val_loss: 1.0305e-04\n",
      "Epoch 21/380\n",
      "631/631 [==============================] - 1373s 2s/step - loss: 4.6940e-05 - val_loss: 6.7974e-05\n",
      "Epoch 22/380\n",
      "631/631 [==============================] - 1366s 2s/step - loss: 4.6586e-05 - val_loss: 5.3234e-05\n",
      "Epoch 23/380\n",
      "631/631 [==============================] - 1365s 2s/step - loss: 4.6018e-05 - val_loss: 6.5902e-05\n",
      "Epoch 24/380\n",
      "631/631 [==============================] - 1366s 2s/step - loss: 4.5588e-05 - val_loss: 5.1319e-05\n",
      "Epoch 25/380\n",
      "631/631 [==============================] - 1360s 2s/step - loss: 4.4624e-05 - val_loss: 7.1176e-05\n",
      "Epoch 26/380\n",
      "631/631 [==============================] - 1361s 2s/step - loss: 4.4883e-05 - val_loss: 5.6682e-05\n",
      "Epoch 27/380\n",
      "631/631 [==============================] - 1360s 2s/step - loss: 4.5235e-05 - val_loss: 7.0180e-05\n",
      "Epoch 28/380\n",
      "631/631 [==============================] - 1322s 2s/step - loss: 4.4919e-05 - val_loss: 5.5054e-05\n",
      "Epoch 29/380\n",
      "631/631 [==============================] - 1317s 2s/step - loss: 4.4715e-05 - val_loss: 4.1742e-05\n",
      "Epoch 30/380\n",
      "631/631 [==============================] - 1313s 2s/step - loss: 4.4412e-05 - val_loss: 5.0513e-05\n",
      "Epoch 31/380\n",
      "631/631 [==============================] - 1312s 2s/step - loss: 4.3515e-05 - val_loss: 6.0110e-05\n",
      "Epoch 32/380\n",
      "631/631 [==============================] - 1316s 2s/step - loss: 4.4334e-05 - val_loss: 6.0736e-05\n",
      "Epoch 33/380\n",
      "631/631 [==============================] - 1315s 2s/step - loss: 4.4491e-05 - val_loss: 4.6973e-05\n",
      "Epoch 34/380\n",
      "631/631 [==============================] - 1320s 2s/step - loss: 4.3701e-05 - val_loss: 4.0408e-05\n",
      "Epoch 35/380\n",
      "631/631 [==============================] - 1321s 2s/step - loss: 4.3245e-05 - val_loss: 4.1767e-05\n",
      "Epoch 36/380\n",
      "631/631 [==============================] - 1316s 2s/step - loss: 4.3614e-05 - val_loss: 4.4374e-05\n",
      "Epoch 37/380\n",
      "631/631 [==============================] - 1315s 2s/step - loss: 4.2867e-05 - val_loss: 4.6217e-05\n",
      "Epoch 38/380\n",
      "631/631 [==============================] - 1315s 2s/step - loss: 4.2711e-05 - val_loss: 8.2063e-05\n",
      "Epoch 39/380\n",
      "631/631 [==============================] - 1310s 2s/step - loss: 4.3149e-05 - val_loss: 4.5552e-05\n",
      "Epoch 40/380\n",
      "631/631 [==============================] - 1292s 2s/step - loss: 4.2029e-05 - val_loss: 4.3271e-05\n",
      "Epoch 41/380\n",
      "631/631 [==============================] - 1288s 2s/step - loss: 4.2335e-05 - val_loss: 4.1788e-05\n",
      "Epoch 42/380\n",
      "631/631 [==============================] - 1289s 2s/step - loss: 4.2881e-05 - val_loss: 4.7404e-05\n",
      "Epoch 43/380\n",
      "631/631 [==============================] - 1303s 2s/step - loss: 4.1657e-05 - val_loss: 4.5854e-05\n",
      "Epoch 44/380\n",
      "631/631 [==============================] - 1288s 2s/step - loss: 4.1469e-05 - val_loss: 4.3846e-05\n",
      "Epoch 45/380\n",
      "631/631 [==============================] - 1281s 2s/step - loss: 4.0947e-05 - val_loss: 4.2650e-05\n",
      "Epoch 46/380\n",
      "631/631 [==============================] - 1289s 2s/step - loss: 4.1614e-05 - val_loss: 4.8250e-05\n",
      "Epoch 47/380\n",
      "631/631 [==============================] - 1294s 2s/step - loss: 4.1068e-05 - val_loss: 3.8552e-05\n",
      "Epoch 48/380\n",
      "631/631 [==============================] - 2483s 4s/step - loss: 4.0620e-05 - val_loss: 3.8644e-05\n",
      "Epoch 49/380\n",
      "631/631 [==============================] - 1402s 2s/step - loss: 4.1602e-05 - val_loss: 6.6695e-05\n",
      "Epoch 50/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 3.9968e-05 - val_loss: 5.7832e-05\n",
      "Epoch 51/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 4.1069e-05 - val_loss: 4.1405e-05\n",
      "Epoch 52/380\n",
      "631/631 [==============================] - 1399s 2s/step - loss: 4.1592e-05 - val_loss: 4.9259e-05\n",
      "Epoch 53/380\n",
      "631/631 [==============================] - 1386s 2s/step - loss: 4.0445e-05 - val_loss: 3.9798e-05\n",
      "Epoch 54/380\n",
      "631/631 [==============================] - 1382s 2s/step - loss: 3.9584e-05 - val_loss: 4.4509e-05\n",
      "Epoch 55/380\n",
      "631/631 [==============================] - 1383s 2s/step - loss: 4.0555e-05 - val_loss: 5.3112e-05\n",
      "Epoch 56/380\n",
      "631/631 [==============================] - 1382s 2s/step - loss: 3.9566e-05 - val_loss: 4.1991e-05\n",
      "Epoch 57/380\n",
      "631/631 [==============================] - 1378s 2s/step - loss: 3.9866e-05 - val_loss: 3.6071e-05\n",
      "Epoch 58/380\n",
      "631/631 [==============================] - 1380s 2s/step - loss: 3.9908e-05 - val_loss: 4.2237e-05\n",
      "Epoch 59/380\n",
      "631/631 [==============================] - 1379s 2s/step - loss: 4.0507e-05 - val_loss: 9.0275e-05\n",
      "Epoch 60/380\n",
      "631/631 [==============================] - 1387s 2s/step - loss: 3.9286e-05 - val_loss: 5.5818e-05\n",
      "Epoch 61/380\n",
      "631/631 [==============================] - 1399s 2s/step - loss: 3.8972e-05 - val_loss: 6.3949e-05\n",
      "Epoch 62/380\n",
      "631/631 [==============================] - 1402s 2s/step - loss: 3.8218e-05 - val_loss: 7.0573e-05\n",
      "Epoch 63/380\n",
      "631/631 [==============================] - 1402s 2s/step - loss: 3.9474e-05 - val_loss: 4.0785e-05\n",
      "Epoch 64/380\n",
      "631/631 [==============================] - 1399s 2s/step - loss: 3.8191e-05 - val_loss: 4.4366e-05\n",
      "Epoch 65/380\n",
      "631/631 [==============================] - 1398s 2s/step - loss: 3.8709e-05 - val_loss: 5.0255e-05\n",
      "Epoch 66/380\n",
      "631/631 [==============================] - 1394s 2s/step - loss: 3.9372e-05 - val_loss: 4.8376e-05\n",
      "Epoch 67/380\n",
      "631/631 [==============================] - 1407s 2s/step - loss: 3.7681e-05 - val_loss: 3.4517e-05\n",
      "Epoch 68/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 3.7699e-05 - val_loss: 6.7251e-05\n",
      "Epoch 69/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 3.8662e-05 - val_loss: 6.7117e-05\n",
      "Epoch 70/380\n",
      "631/631 [==============================] - 1407s 2s/step - loss: 3.7789e-05 - val_loss: 5.2491e-05\n",
      "Epoch 71/380\n",
      "631/631 [==============================] - 1402s 2s/step - loss: 3.6862e-05 - val_loss: 4.7247e-05\n",
      "Epoch 72/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 3.8161e-05 - val_loss: 4.3518e-05\n",
      "Epoch 73/380\n",
      "631/631 [==============================] - 1405s 2s/step - loss: 3.7923e-05 - val_loss: 5.8220e-05\n",
      "Epoch 74/380\n",
      "631/631 [==============================] - 1396s 2s/step - loss: 3.7406e-05 - val_loss: 3.9960e-05\n",
      "Epoch 75/380\n",
      "631/631 [==============================] - 1393s 2s/step - loss: 3.7277e-05 - val_loss: 3.7593e-05\n",
      "Epoch 76/380\n",
      "631/631 [==============================] - 1390s 2s/step - loss: 3.7149e-05 - val_loss: 4.0147e-05\n",
      "Epoch 77/380\n",
      "631/631 [==============================] - 1385s 2s/step - loss: 3.7189e-05 - val_loss: 3.6805e-05\n",
      "Epoch 78/380\n",
      "631/631 [==============================] - 1389s 2s/step - loss: 3.6871e-05 - val_loss: 3.6997e-05\n",
      "Epoch 79/380\n",
      "631/631 [==============================] - 1382s 2s/step - loss: 3.6136e-05 - val_loss: 5.9449e-05\n",
      "Epoch 80/380\n",
      "631/631 [==============================] - 1380s 2s/step - loss: 3.6260e-05 - val_loss: 4.8138e-05\n",
      "Epoch 81/380\n",
      "631/631 [==============================] - 1382s 2s/step - loss: 3.6459e-05 - val_loss: 4.0642e-05\n",
      "Epoch 82/380\n",
      "631/631 [==============================] - 1388s 2s/step - loss: 3.6026e-05 - val_loss: 3.4490e-05\n",
      "Epoch 83/380\n",
      "631/631 [==============================] - 1376s 2s/step - loss: 3.6280e-05 - val_loss: 3.6204e-05\n",
      "Epoch 84/380\n",
      "631/631 [==============================] - 1374s 2s/step - loss: 3.6429e-05 - val_loss: 3.8288e-05\n",
      "Epoch 85/380\n",
      "631/631 [==============================] - 1378s 2s/step - loss: 3.6616e-05 - val_loss: 3.6511e-05\n",
      "Epoch 86/380\n",
      "631/631 [==============================] - 1366s 2s/step - loss: 3.5534e-05 - val_loss: 4.2701e-05\n",
      "Epoch 87/380\n",
      "631/631 [==============================] - 1370s 2s/step - loss: 3.6167e-05 - val_loss: 3.9382e-05\n",
      "Epoch 88/380\n",
      "631/631 [==============================] - 1371s 2s/step - loss: 3.6211e-05 - val_loss: 3.6734e-05\n",
      "Epoch 89/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 3.6293e-05 - val_loss: 6.1162e-05\n",
      "Epoch 90/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 3.5607e-05 - val_loss: 3.4880e-05\n",
      "Epoch 91/380\n",
      "631/631 [==============================] - 1371s 2s/step - loss: 3.5437e-05 - val_loss: 6.3736e-05\n",
      "Epoch 92/380\n",
      "631/631 [==============================] - 1376s 2s/step - loss: 3.5282e-05 - val_loss: 3.7199e-05\n",
      "Epoch 93/380\n",
      "631/631 [==============================] - 1378s 2s/step - loss: 3.4613e-05 - val_loss: 3.7371e-05\n",
      "Epoch 94/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 3.4585e-05 - val_loss: 7.1405e-05\n",
      "Epoch 95/380\n",
      "631/631 [==============================] - 1368s 2s/step - loss: 3.6197e-05 - val_loss: 3.3585e-05\n",
      "Epoch 96/380\n",
      "631/631 [==============================] - 1367s 2s/step - loss: 3.4558e-05 - val_loss: 3.5010e-05\n",
      "Epoch 97/380\n",
      "631/631 [==============================] - 1372s 2s/step - loss: 3.4738e-05 - val_loss: 5.3478e-05\n",
      "Epoch 98/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 3.4851e-05 - val_loss: 3.4449e-05\n",
      "Epoch 99/380\n",
      "631/631 [==============================] - 1372s 2s/step - loss: 3.4410e-05 - val_loss: 3.5763e-05\n",
      "Epoch 100/380\n",
      "631/631 [==============================] - 1372s 2s/step - loss: 3.4111e-05 - val_loss: 3.7702e-05\n",
      "Epoch 101/380\n",
      "631/631 [==============================] - 1371s 2s/step - loss: 3.4471e-05 - val_loss: 4.4346e-05\n",
      "Epoch 102/380\n",
      "631/631 [==============================] - 1369s 2s/step - loss: 3.3938e-05 - val_loss: 3.8622e-05\n",
      "Epoch 103/380\n",
      "631/631 [==============================] - 1373s 2s/step - loss: 3.4073e-05 - val_loss: 4.4013e-05\n",
      "Epoch 104/380\n",
      "631/631 [==============================] - 1386s 2s/step - loss: 3.4375e-05 - val_loss: 4.3091e-05\n",
      "Epoch 105/380\n",
      "631/631 [==============================] - 1400s 2s/step - loss: 3.3170e-05 - val_loss: 3.1214e-05\n",
      "Epoch 106/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 3.4062e-05 - val_loss: 3.6204e-05\n",
      "Epoch 107/380\n",
      "631/631 [==============================] - 1405s 2s/step - loss: 3.4224e-05 - val_loss: 4.2872e-05\n",
      "Epoch 108/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 3.3286e-05 - val_loss: 3.4147e-05\n",
      "Epoch 109/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 3.3412e-05 - val_loss: 4.5024e-05\n",
      "Epoch 110/380\n",
      "631/631 [==============================] - 1417s 2s/step - loss: 3.3366e-05 - val_loss: 4.3707e-05\n",
      "Epoch 111/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 3.2905e-05 - val_loss: 3.0256e-05\n",
      "Epoch 112/380\n",
      "631/631 [==============================] - 1418s 2s/step - loss: 3.2736e-05 - val_loss: 3.6096e-05\n",
      "Epoch 113/380\n",
      "631/631 [==============================] - 1417s 2s/step - loss: 3.3187e-05 - val_loss: 3.6083e-05\n",
      "Epoch 114/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 3.3185e-05 - val_loss: 2.9595e-05\n",
      "Epoch 115/380\n",
      "631/631 [==============================] - 1412s 2s/step - loss: 3.2664e-05 - val_loss: 4.6734e-05\n",
      "Epoch 116/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 3.2702e-05 - val_loss: 3.6519e-05\n",
      "Epoch 117/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 3.2888e-05 - val_loss: 3.3710e-05\n",
      "Epoch 118/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 3.2768e-05 - val_loss: 3.1033e-05\n",
      "Epoch 119/380\n",
      "631/631 [==============================] - 1418s 2s/step - loss: 3.2103e-05 - val_loss: 6.8220e-05\n",
      "Epoch 120/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 3.2893e-05 - val_loss: 2.9399e-05\n",
      "Epoch 121/380\n",
      "631/631 [==============================] - 1429s 2s/step - loss: 3.1882e-05 - val_loss: 2.9814e-05\n",
      "Epoch 122/380\n",
      "631/631 [==============================] - 1429s 2s/step - loss: 3.1683e-05 - val_loss: 5.5152e-05\n",
      "Epoch 123/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 3.2269e-05 - val_loss: 3.4418e-05\n",
      "Epoch 124/380\n",
      "631/631 [==============================] - 1423s 2s/step - loss: 3.2174e-05 - val_loss: 3.6220e-05\n",
      "Epoch 125/380\n",
      "631/631 [==============================] - 1433s 2s/step - loss: 3.1784e-05 - val_loss: 5.2912e-05\n",
      "Epoch 126/380\n",
      "631/631 [==============================] - 1430s 2s/step - loss: 3.1772e-05 - val_loss: 4.0655e-05\n",
      "Epoch 127/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 3.2101e-05 - val_loss: 4.5805e-05\n",
      "Epoch 128/380\n",
      "631/631 [==============================] - 1430s 2s/step - loss: 3.2092e-05 - val_loss: 2.8791e-05\n",
      "Epoch 129/380\n",
      "631/631 [==============================] - 1435s 2s/step - loss: 3.1964e-05 - val_loss: 4.0162e-05\n",
      "Epoch 130/380\n",
      "631/631 [==============================] - 1427s 2s/step - loss: 3.1206e-05 - val_loss: 3.3087e-05\n",
      "Epoch 131/380\n",
      "631/631 [==============================] - 1428s 2s/step - loss: 3.2497e-05 - val_loss: 3.8136e-05\n",
      "Epoch 132/380\n",
      "631/631 [==============================] - 1428s 2s/step - loss: 3.1290e-05 - val_loss: 3.2962e-05\n",
      "Epoch 133/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 3.0755e-05 - val_loss: 3.2215e-05\n",
      "Epoch 134/380\n",
      "631/631 [==============================] - 1430s 2s/step - loss: 3.1147e-05 - val_loss: 3.3801e-05\n",
      "Epoch 135/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 3.1181e-05 - val_loss: 4.0611e-05\n",
      "Epoch 136/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 3.1300e-05 - val_loss: 3.6058e-05\n",
      "Epoch 137/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 3.0625e-05 - val_loss: 6.0983e-05\n",
      "Epoch 138/380\n",
      "631/631 [==============================] - 1415s 2s/step - loss: 3.0550e-05 - val_loss: 3.3372e-05\n",
      "Epoch 139/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 3.0933e-05 - val_loss: 3.0080e-05\n",
      "Epoch 140/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 3.1874e-05 - val_loss: 3.8884e-05\n",
      "Epoch 141/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 3.0889e-05 - val_loss: 3.9278e-05\n",
      "Epoch 142/380\n",
      "631/631 [==============================] - 1407s 2s/step - loss: 3.0908e-05 - val_loss: 3.4662e-05\n",
      "Epoch 143/380\n",
      "631/631 [==============================] - 1405s 2s/step - loss: 3.0392e-05 - val_loss: 3.0440e-05\n",
      "Epoch 144/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 3.0484e-05 - val_loss: 4.1720e-05\n",
      "Epoch 145/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 3.0487e-05 - val_loss: 3.5557e-05\n",
      "Epoch 146/380\n",
      "631/631 [==============================] - 1408s 2s/step - loss: 3.0714e-05 - val_loss: 3.5017e-05\n",
      "Epoch 147/380\n",
      "631/631 [==============================] - 1404s 2s/step - loss: 3.0343e-05 - val_loss: 4.4699e-05\n",
      "Epoch 148/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 3.0170e-05 - val_loss: 3.5511e-05\n",
      "Epoch 149/380\n",
      "631/631 [==============================] - 1403s 2s/step - loss: 2.9990e-05 - val_loss: 4.0628e-05\n",
      "Epoch 150/380\n",
      "631/631 [==============================] - 1396s 2s/step - loss: 3.0606e-05 - val_loss: 3.3897e-05\n",
      "Epoch 151/380\n",
      "631/631 [==============================] - 1395s 2s/step - loss: 2.9896e-05 - val_loss: 4.0551e-05\n",
      "Epoch 152/380\n",
      "631/631 [==============================] - 1395s 2s/step - loss: 2.9406e-05 - val_loss: 3.2884e-05\n",
      "Epoch 153/380\n",
      "631/631 [==============================] - 1395s 2s/step - loss: 3.0040e-05 - val_loss: 3.1239e-05\n",
      "Epoch 154/380\n",
      "631/631 [==============================] - 1398s 2s/step - loss: 2.9806e-05 - val_loss: 4.2356e-05\n",
      "Epoch 155/380\n",
      "631/631 [==============================] - 1397s 2s/step - loss: 2.9841e-05 - val_loss: 3.8941e-05\n",
      "Epoch 156/380\n",
      "631/631 [==============================] - 1388s 2s/step - loss: 2.9603e-05 - val_loss: 3.2729e-05\n",
      "Epoch 157/380\n",
      "631/631 [==============================] - 1394s 2s/step - loss: 2.9613e-05 - val_loss: 3.3154e-05\n",
      "Epoch 158/380\n",
      "631/631 [==============================] - 1393s 2s/step - loss: 2.9897e-05 - val_loss: 3.0009e-05\n",
      "Epoch 159/380\n",
      "631/631 [==============================] - 1397s 2s/step - loss: 2.9638e-05 - val_loss: 3.3546e-05\n",
      "Epoch 160/380\n",
      "631/631 [==============================] - 1394s 2s/step - loss: 2.9501e-05 - val_loss: 2.9688e-05\n",
      "Epoch 161/380\n",
      "631/631 [==============================] - 1390s 2s/step - loss: 2.9392e-05 - val_loss: 2.6137e-05\n",
      "Epoch 162/380\n",
      "631/631 [==============================] - 1402s 2s/step - loss: 2.8889e-05 - val_loss: 4.9715e-05\n",
      "Epoch 163/380\n",
      "631/631 [==============================] - 1401s 2s/step - loss: 2.9202e-05 - val_loss: 3.9903e-05\n",
      "Epoch 164/380\n",
      "631/631 [==============================] - 1395s 2s/step - loss: 2.9029e-05 - val_loss: 3.0947e-05\n",
      "Epoch 165/380\n",
      "631/631 [==============================] - 1400s 2s/step - loss: 2.8838e-05 - val_loss: 3.1012e-05\n",
      "Epoch 166/380\n",
      "631/631 [==============================] - 1404s 2s/step - loss: 2.8900e-05 - val_loss: 3.0629e-05\n",
      "Epoch 167/380\n",
      "631/631 [==============================] - 1411s 2s/step - loss: 2.9423e-05 - val_loss: 2.8242e-05\n",
      "Epoch 168/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.8854e-05 - val_loss: 3.1070e-05\n",
      "Epoch 169/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 2.8882e-05 - val_loss: 3.1978e-05\n",
      "Epoch 170/380\n",
      "631/631 [==============================] - 1415s 2s/step - loss: 2.8892e-05 - val_loss: 3.4491e-05\n",
      "Epoch 171/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 2.8986e-05 - val_loss: 4.1336e-05\n",
      "Epoch 172/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.9131e-05 - val_loss: 3.5701e-05\n",
      "Epoch 173/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.8507e-05 - val_loss: 3.0727e-05\n",
      "Epoch 174/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.8799e-05 - val_loss: 2.7687e-05\n",
      "Epoch 175/380\n",
      "631/631 [==============================] - 1412s 2s/step - loss: 2.8599e-05 - val_loss: 3.2956e-05\n",
      "Epoch 176/380\n",
      "631/631 [==============================] - 1411s 2s/step - loss: 2.9266e-05 - val_loss: 2.6574e-05\n",
      "Epoch 177/380\n",
      "631/631 [==============================] - 1411s 2s/step - loss: 2.8081e-05 - val_loss: 3.6747e-05\n",
      "Epoch 178/380\n",
      "631/631 [==============================] - 1423s 2s/step - loss: 2.8532e-05 - val_loss: 2.8307e-05\n",
      "Epoch 179/380\n",
      "631/631 [==============================] - 1438s 2s/step - loss: 2.8278e-05 - val_loss: 3.4383e-05\n",
      "Epoch 180/380\n",
      "631/631 [==============================] - 1439s 2s/step - loss: 2.8172e-05 - val_loss: 5.5734e-05\n",
      "Epoch 181/380\n",
      "631/631 [==============================] - 1432s 2s/step - loss: 2.7963e-05 - val_loss: 6.8182e-05\n",
      "Epoch 182/380\n",
      "631/631 [==============================] - 1433s 2s/step - loss: 2.8088e-05 - val_loss: 3.0553e-05\n",
      "Epoch 183/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.8326e-05 - val_loss: 2.7629e-05\n",
      "Epoch 184/380\n",
      "631/631 [==============================] - 1443s 2s/step - loss: 2.8079e-05 - val_loss: 4.9860e-05\n",
      "Epoch 185/380\n",
      "631/631 [==============================] - 1453s 2s/step - loss: 2.7485e-05 - val_loss: 2.4819e-05\n",
      "Epoch 186/380\n",
      "631/631 [==============================] - 1444s 2s/step - loss: 2.8383e-05 - val_loss: 2.7349e-05\n",
      "Epoch 187/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.8004e-05 - val_loss: 3.9478e-05\n",
      "Epoch 188/380\n",
      "631/631 [==============================] - 1452s 2s/step - loss: 2.7472e-05 - val_loss: 3.4288e-05\n",
      "Epoch 189/380\n",
      "631/631 [==============================] - 1457s 2s/step - loss: 2.7954e-05 - val_loss: 3.1369e-05\n",
      "Epoch 190/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.7411e-05 - val_loss: 2.8885e-05\n",
      "Epoch 191/380\n",
      "631/631 [==============================] - 1465s 2s/step - loss: 2.7431e-05 - val_loss: 3.6564e-05\n",
      "Epoch 192/380\n",
      "631/631 [==============================] - 1468s 2s/step - loss: 2.7593e-05 - val_loss: 2.9304e-05\n",
      "Epoch 193/380\n",
      "631/631 [==============================] - 1460s 2s/step - loss: 2.7593e-05 - val_loss: 2.8469e-05\n",
      "Epoch 194/380\n",
      "631/631 [==============================] - 1461s 2s/step - loss: 2.7307e-05 - val_loss: 2.5388e-05\n",
      "Epoch 195/380\n",
      "631/631 [==============================] - 1461s 2s/step - loss: 2.7943e-05 - val_loss: 2.5402e-05\n",
      "Epoch 196/380\n",
      "631/631 [==============================] - 1460s 2s/step - loss: 2.7751e-05 - val_loss: 2.5242e-05\n",
      "Epoch 197/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.7243e-05 - val_loss: 3.0065e-05\n",
      "Epoch 198/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.6419e-05 - val_loss: 3.1087e-05\n",
      "Epoch 199/380\n",
      "631/631 [==============================] - 1447s 2s/step - loss: 2.7194e-05 - val_loss: 3.0249e-05\n",
      "Epoch 200/380\n",
      "631/631 [==============================] - 1451s 2s/step - loss: 2.6709e-05 - val_loss: 3.2278e-05\n",
      "Epoch 201/380\n",
      "631/631 [==============================] - 1445s 2s/step - loss: 2.7198e-05 - val_loss: 3.2878e-05\n",
      "Epoch 202/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.7321e-05 - val_loss: 2.8729e-05\n",
      "Epoch 203/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.6693e-05 - val_loss: 2.8457e-05\n",
      "Epoch 204/380\n",
      "631/631 [==============================] - 1438s 2s/step - loss: 2.6823e-05 - val_loss: 2.4052e-05\n",
      "Epoch 205/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.7389e-05 - val_loss: 3.6551e-05\n",
      "Epoch 206/380\n",
      "631/631 [==============================] - 1442s 2s/step - loss: 2.6725e-05 - val_loss: 3.6098e-05\n",
      "Epoch 207/380\n",
      "631/631 [==============================] - 1442s 2s/step - loss: 2.6886e-05 - val_loss: 3.7337e-05\n",
      "Epoch 208/380\n",
      "631/631 [==============================] - 1438s 2s/step - loss: 2.7094e-05 - val_loss: 2.9109e-05\n",
      "Epoch 209/380\n",
      "631/631 [==============================] - 1437s 2s/step - loss: 2.6566e-05 - val_loss: 2.9118e-05\n",
      "Epoch 210/380\n",
      "631/631 [==============================] - 1439s 2s/step - loss: 2.6328e-05 - val_loss: 2.8622e-05\n",
      "Epoch 211/380\n",
      "631/631 [==============================] - 1442s 2s/step - loss: 2.6477e-05 - val_loss: 3.2036e-05\n",
      "Epoch 212/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.6552e-05 - val_loss: 2.7843e-05\n",
      "Epoch 213/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 2.6457e-05 - val_loss: 2.8856e-05\n",
      "Epoch 214/380\n",
      "631/631 [==============================] - 1430s 2s/step - loss: 2.6299e-05 - val_loss: 2.8416e-05\n",
      "Epoch 215/380\n",
      "631/631 [==============================] - 1429s 2s/step - loss: 2.6590e-05 - val_loss: 3.0946e-05\n",
      "Epoch 216/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.6271e-05 - val_loss: 2.4846e-05\n",
      "Epoch 217/380\n",
      "631/631 [==============================] - 1424s 2s/step - loss: 2.6229e-05 - val_loss: 3.3124e-05\n",
      "Epoch 218/380\n",
      "631/631 [==============================] - 1423s 2s/step - loss: 2.6487e-05 - val_loss: 2.9456e-05\n",
      "Epoch 219/380\n",
      "631/631 [==============================] - 1426s 2s/step - loss: 2.6186e-05 - val_loss: 3.5111e-05\n",
      "Epoch 220/380\n",
      "631/631 [==============================] - 1424s 2s/step - loss: 2.6343e-05 - val_loss: 4.3429e-05\n",
      "Epoch 221/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.6481e-05 - val_loss: 3.0711e-05\n",
      "Epoch 222/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.5978e-05 - val_loss: 2.9033e-05\n",
      "Epoch 223/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 2.6121e-05 - val_loss: 2.9087e-05\n",
      "Epoch 224/380\n",
      "631/631 [==============================] - 1439s 2s/step - loss: 2.6016e-05 - val_loss: 2.5405e-05\n",
      "Epoch 225/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.5930e-05 - val_loss: 4.3533e-05\n",
      "Epoch 226/380\n",
      "631/631 [==============================] - 1407s 2s/step - loss: 2.6303e-05 - val_loss: 3.1623e-05\n",
      "Epoch 227/380\n",
      "631/631 [==============================] - 1415s 2s/step - loss: 2.5462e-05 - val_loss: 2.8952e-05\n",
      "Epoch 228/380\n",
      "631/631 [==============================] - 1408s 2s/step - loss: 2.5581e-05 - val_loss: 2.9077e-05\n",
      "Epoch 229/380\n",
      "631/631 [==============================] - 1401s 2s/step - loss: 2.5328e-05 - val_loss: 2.8291e-05\n",
      "Epoch 230/380\n",
      "631/631 [==============================] - 1399s 2s/step - loss: 2.5807e-05 - val_loss: 2.6643e-05\n",
      "Epoch 231/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.5592e-05 - val_loss: 3.2059e-05\n",
      "Epoch 232/380\n",
      "631/631 [==============================] - 1515s 2s/step - loss: 2.5658e-05 - val_loss: 2.5183e-05\n",
      "Epoch 233/380\n",
      "631/631 [==============================] - 1370s 2s/step - loss: 2.5543e-05 - val_loss: 4.4123e-05\n",
      "Epoch 234/380\n",
      "631/631 [==============================] - 1384s 2s/step - loss: 2.5420e-05 - val_loss: 2.4959e-05\n",
      "Epoch 235/380\n",
      "631/631 [==============================] - 1388s 2s/step - loss: 2.5473e-05 - val_loss: 3.7551e-05\n",
      "Epoch 236/380\n",
      "631/631 [==============================] - 1389s 2s/step - loss: 2.5510e-05 - val_loss: 3.1213e-05\n",
      "Epoch 237/380\n",
      "631/631 [==============================] - 1392s 2s/step - loss: 2.5454e-05 - val_loss: 2.7042e-05\n",
      "Epoch 238/380\n",
      "631/631 [==============================] - 1394s 2s/step - loss: 2.5247e-05 - val_loss: 3.0952e-05\n",
      "Epoch 239/380\n",
      "631/631 [==============================] - 1389s 2s/step - loss: 2.5309e-05 - val_loss: 2.5310e-05\n",
      "Epoch 240/380\n",
      "631/631 [==============================] - 1396s 2s/step - loss: 2.5232e-05 - val_loss: 2.7049e-05\n",
      "Epoch 241/380\n",
      "631/631 [==============================] - 1396s 2s/step - loss: 2.5101e-05 - val_loss: 2.6563e-05\n",
      "Epoch 242/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.4792e-05 - val_loss: 2.4311e-05\n",
      "Epoch 243/380\n",
      "631/631 [==============================] - 1428s 2s/step - loss: 2.6036e-05 - val_loss: 3.4865e-05\n",
      "Epoch 244/380\n",
      "631/631 [==============================] - 1426s 2s/step - loss: 2.5149e-05 - val_loss: 2.5332e-05\n",
      "Epoch 245/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.5111e-05 - val_loss: 3.2998e-05\n",
      "Epoch 246/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.4991e-05 - val_loss: 2.9938e-05\n",
      "Epoch 247/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.5194e-05 - val_loss: 2.8441e-05\n",
      "Epoch 248/380\n",
      "631/631 [==============================] - 1443s 2s/step - loss: 2.5188e-05 - val_loss: 4.2058e-05\n",
      "Epoch 249/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.5233e-05 - val_loss: 2.8503e-05\n",
      "Epoch 250/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.4944e-05 - val_loss: 3.0392e-05\n",
      "Epoch 251/380\n",
      "631/631 [==============================] - 1444s 2s/step - loss: 2.5158e-05 - val_loss: 2.4153e-05\n",
      "Epoch 252/380\n",
      "631/631 [==============================] - 1445s 2s/step - loss: 2.4543e-05 - val_loss: 2.5983e-05\n",
      "Epoch 253/380\n",
      "631/631 [==============================] - 1445s 2s/step - loss: 2.4911e-05 - val_loss: 2.8237e-05\n",
      "Epoch 254/380\n",
      "631/631 [==============================] - 1453s 2s/step - loss: 2.4250e-05 - val_loss: 4.1954e-05\n",
      "Epoch 255/380\n",
      "631/631 [==============================] - 1451s 2s/step - loss: 2.4502e-05 - val_loss: 2.7752e-05\n",
      "Epoch 256/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.4487e-05 - val_loss: 3.2421e-05\n",
      "Epoch 257/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.4834e-05 - val_loss: 4.1375e-05\n",
      "Epoch 258/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.4538e-05 - val_loss: 2.9873e-05\n",
      "Epoch 259/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.4781e-05 - val_loss: 3.5002e-05\n",
      "Epoch 260/380\n",
      "631/631 [==============================] - 1437s 2s/step - loss: 2.4060e-05 - val_loss: 3.7895e-05\n",
      "Epoch 261/380\n",
      "631/631 [==============================] - 1433s 2s/step - loss: 2.4914e-05 - val_loss: 3.1408e-05\n",
      "Epoch 262/380\n",
      "631/631 [==============================] - 1426s 2s/step - loss: 2.4101e-05 - val_loss: 3.7172e-05\n",
      "Epoch 263/380\n",
      "631/631 [==============================] - 1439s 2s/step - loss: 2.4308e-05 - val_loss: 2.7177e-05\n",
      "Epoch 264/380\n",
      "631/631 [==============================] - 1427s 2s/step - loss: 2.4131e-05 - val_loss: 2.1891e-05\n",
      "Epoch 265/380\n",
      "631/631 [==============================] - 1428s 2s/step - loss: 2.4124e-05 - val_loss: 3.2679e-05\n",
      "Epoch 266/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 2.3929e-05 - val_loss: 3.2182e-05\n",
      "Epoch 267/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.3970e-05 - val_loss: 2.2343e-05\n",
      "Epoch 268/380\n",
      "631/631 [==============================] - 1422s 2s/step - loss: 2.4703e-05 - val_loss: 2.3665e-05\n",
      "Epoch 269/380\n",
      "631/631 [==============================] - 1426s 2s/step - loss: 2.4352e-05 - val_loss: 3.6377e-05\n",
      "Epoch 270/380\n",
      "631/631 [==============================] - 1424s 2s/step - loss: 2.3996e-05 - val_loss: 2.7521e-05\n",
      "Epoch 271/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.3775e-05 - val_loss: 3.5650e-05\n",
      "Epoch 272/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.4063e-05 - val_loss: 3.2539e-05\n",
      "Epoch 273/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.3776e-05 - val_loss: 3.7180e-05\n",
      "Epoch 274/380\n",
      "631/631 [==============================] - 1418s 2s/step - loss: 2.4214e-05 - val_loss: 2.6998e-05\n",
      "Epoch 275/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.3841e-05 - val_loss: 2.4422e-05\n",
      "Epoch 276/380\n",
      "631/631 [==============================] - 1422s 2s/step - loss: 2.3853e-05 - val_loss: 3.9289e-05\n",
      "Epoch 277/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 2.3916e-05 - val_loss: 2.9756e-05\n",
      "Epoch 278/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.4289e-05 - val_loss: 3.7449e-05\n",
      "Epoch 279/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 2.3689e-05 - val_loss: 2.8571e-05\n",
      "Epoch 280/380\n",
      "631/631 [==============================] - 1412s 2s/step - loss: 2.3551e-05 - val_loss: 2.9434e-05\n",
      "Epoch 281/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 2.3473e-05 - val_loss: 2.5891e-05\n",
      "Epoch 282/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 2.3985e-05 - val_loss: 2.1066e-05\n",
      "Epoch 283/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 2.4385e-05 - val_loss: 2.2686e-05\n",
      "Epoch 284/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 2.3900e-05 - val_loss: 2.7848e-05\n",
      "Epoch 285/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 2.3629e-05 - val_loss: 2.6277e-05\n",
      "Epoch 286/380\n",
      "631/631 [==============================] - 1400s 2s/step - loss: 2.3584e-05 - val_loss: 2.5772e-05\n",
      "Epoch 287/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 2.3460e-05 - val_loss: 2.9763e-05\n",
      "Epoch 288/380\n",
      "631/631 [==============================] - 1406s 2s/step - loss: 2.3162e-05 - val_loss: 2.7370e-05\n",
      "Epoch 289/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.3709e-05 - val_loss: 2.0901e-05\n",
      "Epoch 290/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 2.3585e-05 - val_loss: 2.8283e-05\n",
      "Epoch 291/380\n",
      "631/631 [==============================] - 1408s 2s/step - loss: 2.3492e-05 - val_loss: 4.1499e-05\n",
      "Epoch 292/380\n",
      "631/631 [==============================] - 1412s 2s/step - loss: 2.3529e-05 - val_loss: 3.3225e-05\n",
      "Epoch 293/380\n",
      "631/631 [==============================] - 1410s 2s/step - loss: 2.3358e-05 - val_loss: 3.5962e-05\n",
      "Epoch 294/380\n",
      "631/631 [==============================] - 1411s 2s/step - loss: 2.3119e-05 - val_loss: 3.3610e-05\n",
      "Epoch 295/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.3621e-05 - val_loss: 2.3671e-05\n",
      "Epoch 296/380\n",
      "631/631 [==============================] - 1415s 2s/step - loss: 2.3656e-05 - val_loss: 3.2721e-05\n",
      "Epoch 297/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 2.3103e-05 - val_loss: 2.4129e-05\n",
      "Epoch 298/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.3059e-05 - val_loss: 2.5318e-05\n",
      "Epoch 299/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.3060e-05 - val_loss: 2.4776e-05\n",
      "Epoch 300/380\n",
      "631/631 [==============================] - 1411s 2s/step - loss: 2.2715e-05 - val_loss: 2.2633e-05\n",
      "Epoch 301/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 2.3226e-05 - val_loss: 2.5879e-05\n",
      "Epoch 302/380\n",
      "631/631 [==============================] - 1417s 2s/step - loss: 2.2874e-05 - val_loss: 2.2017e-05\n",
      "Epoch 303/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.3324e-05 - val_loss: 2.4648e-05\n",
      "Epoch 304/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.2945e-05 - val_loss: 2.5079e-05\n",
      "Epoch 305/380\n",
      "631/631 [==============================] - 1449s 2s/step - loss: 2.3050e-05 - val_loss: 3.0054e-05\n",
      "Epoch 306/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.3060e-05 - val_loss: 2.6424e-05\n",
      "Epoch 307/380\n",
      "631/631 [==============================] - 1440s 2s/step - loss: 2.2688e-05 - val_loss: 2.5214e-05\n",
      "Epoch 308/380\n",
      "631/631 [==============================] - 1437s 2s/step - loss: 2.2755e-05 - val_loss: 2.8516e-05\n",
      "Epoch 309/380\n",
      "631/631 [==============================] - 1443s 2s/step - loss: 2.2612e-05 - val_loss: 2.5823e-05\n",
      "Epoch 310/380\n",
      "631/631 [==============================] - 1445s 2s/step - loss: 2.2843e-05 - val_loss: 2.4209e-05\n",
      "Epoch 311/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.2739e-05 - val_loss: 3.5661e-05\n",
      "Epoch 312/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.2401e-05 - val_loss: 2.4108e-05\n",
      "Epoch 313/380\n",
      "631/631 [==============================] - 1450s 2s/step - loss: 2.2406e-05 - val_loss: 2.3469e-05\n",
      "Epoch 314/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.2513e-05 - val_loss: 2.5119e-05\n",
      "Epoch 315/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.2371e-05 - val_loss: 2.4302e-05\n",
      "Epoch 316/380\n",
      "631/631 [==============================] - 1450s 2s/step - loss: 2.2509e-05 - val_loss: 2.5948e-05\n",
      "Epoch 317/380\n",
      "631/631 [==============================] - 1452s 2s/step - loss: 2.2635e-05 - val_loss: 2.5817e-05\n",
      "Epoch 318/380\n",
      "631/631 [==============================] - 1455s 2s/step - loss: 2.2918e-05 - val_loss: 3.3073e-05\n",
      "Epoch 319/380\n",
      "631/631 [==============================] - 1457s 2s/step - loss: 2.2136e-05 - val_loss: 2.0421e-05\n",
      "Epoch 320/380\n",
      "631/631 [==============================] - 1450s 2s/step - loss: 2.2659e-05 - val_loss: 2.8544e-05\n",
      "Epoch 321/380\n",
      "631/631 [==============================] - 1445s 2s/step - loss: 2.3036e-05 - val_loss: 2.7726e-05\n",
      "Epoch 322/380\n",
      "631/631 [==============================] - 1437s 2s/step - loss: 2.2516e-05 - val_loss: 2.6592e-05\n",
      "Epoch 323/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.2654e-05 - val_loss: 3.3887e-05\n",
      "Epoch 324/380\n",
      "631/631 [==============================] - 1441s 2s/step - loss: 2.2533e-05 - val_loss: 2.2041e-05\n",
      "Epoch 325/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.2144e-05 - val_loss: 2.1150e-05\n",
      "Epoch 326/380\n",
      "631/631 [==============================] - 1435s 2s/step - loss: 2.2304e-05 - val_loss: 2.8863e-05\n",
      "Epoch 327/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.2254e-05 - val_loss: 2.1278e-05\n",
      "Epoch 328/380\n",
      "631/631 [==============================] - 1433s 2s/step - loss: 2.2521e-05 - val_loss: 2.0966e-05\n",
      "Epoch 329/380\n",
      "631/631 [==============================] - 1437s 2s/step - loss: 2.2596e-05 - val_loss: 2.5083e-05\n",
      "Epoch 330/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.2417e-05 - val_loss: 2.3249e-05\n",
      "Epoch 331/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 2.2070e-05 - val_loss: 2.6024e-05\n",
      "Epoch 332/380\n",
      "631/631 [==============================] - 1432s 2s/step - loss: 2.2160e-05 - val_loss: 2.4658e-05\n",
      "Epoch 333/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 2.1961e-05 - val_loss: 3.0187e-05\n",
      "Epoch 334/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.1874e-05 - val_loss: 2.5228e-05\n",
      "Epoch 335/380\n",
      "631/631 [==============================] - 1440s 2s/step - loss: 2.2062e-05 - val_loss: 2.2978e-05\n",
      "Epoch 336/380\n",
      "631/631 [==============================] - 1435s 2s/step - loss: 2.2487e-05 - val_loss: 2.3642e-05\n",
      "Epoch 337/380\n",
      "631/631 [==============================] - 1438s 2s/step - loss: 2.1989e-05 - val_loss: 2.2644e-05\n",
      "Epoch 338/380\n",
      "631/631 [==============================] - 1436s 2s/step - loss: 2.2067e-05 - val_loss: 2.2131e-05\n",
      "Epoch 339/380\n",
      "631/631 [==============================] - 1444s 2s/step - loss: 2.1911e-05 - val_loss: 2.4738e-05\n",
      "Epoch 340/380\n",
      "631/631 [==============================] - 1439s 2s/step - loss: 2.1677e-05 - val_loss: 2.2067e-05\n",
      "Epoch 341/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.1933e-05 - val_loss: 2.6802e-05\n",
      "Epoch 342/380\n",
      "631/631 [==============================] - 1434s 2s/step - loss: 2.1967e-05 - val_loss: 4.5371e-05\n",
      "Epoch 343/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 2.2276e-05 - val_loss: 2.5823e-05\n",
      "Epoch 344/380\n",
      "631/631 [==============================] - 1417s 2s/step - loss: 2.1685e-05 - val_loss: 2.1317e-05\n",
      "Epoch 345/380\n",
      "631/631 [==============================] - 1420s 2s/step - loss: 2.1937e-05 - val_loss: 2.1498e-05\n",
      "Epoch 346/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.1658e-05 - val_loss: 2.9344e-05\n",
      "Epoch 347/380\n",
      "631/631 [==============================] - 1419s 2s/step - loss: 2.2284e-05 - val_loss: 2.5511e-05\n",
      "Epoch 348/380\n",
      "631/631 [==============================] - 1425s 2s/step - loss: 2.1555e-05 - val_loss: 2.7780e-05\n",
      "Epoch 349/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.2039e-05 - val_loss: 2.3466e-05\n",
      "Epoch 350/380\n",
      "631/631 [==============================] - 1413s 2s/step - loss: 2.1561e-05 - val_loss: 3.3544e-05\n",
      "Epoch 351/380\n",
      "631/631 [==============================] - 1416s 2s/step - loss: 2.1488e-05 - val_loss: 2.5096e-05\n",
      "Epoch 352/380\n",
      "631/631 [==============================] - 1422s 2s/step - loss: 2.1671e-05 - val_loss: 2.7281e-05\n",
      "Epoch 353/380\n",
      "631/631 [==============================] - 1426s 2s/step - loss: 2.1954e-05 - val_loss: 1.9759e-05\n",
      "Epoch 354/380\n",
      "631/631 [==============================] - 1415s 2s/step - loss: 2.1565e-05 - val_loss: 2.6430e-05\n",
      "Epoch 355/380\n",
      "631/631 [==============================] - 1414s 2s/step - loss: 2.1744e-05 - val_loss: 2.4097e-05\n",
      "Epoch 356/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 2.1725e-05 - val_loss: 2.2472e-05\n",
      "Epoch 357/380\n",
      "631/631 [==============================] - 1429s 2s/step - loss: 2.1621e-05 - val_loss: 2.2200e-05\n",
      "Epoch 358/380\n",
      "631/631 [==============================] - 1431s 2s/step - loss: 2.1482e-05 - val_loss: 2.4125e-05\n",
      "Epoch 359/380\n",
      "631/631 [==============================] - 1440s 2s/step - loss: 2.1728e-05 - val_loss: 2.5003e-05\n",
      "Epoch 360/380\n",
      "631/631 [==============================] - 1423s 2s/step - loss: 2.1559e-05 - val_loss: 3.4641e-05\n",
      "Epoch 361/380\n",
      "631/631 [==============================] - 1421s 2s/step - loss: 2.1453e-05 - val_loss: 2.1680e-05\n",
      "Epoch 362/380\n",
      "631/631 [==============================] - 1424s 2s/step - loss: 2.1401e-05 - val_loss: 2.2919e-05\n",
      "Epoch 363/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.1191e-05 - val_loss: 2.7901e-05\n",
      "Epoch 364/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.1145e-05 - val_loss: 2.2287e-05\n",
      "Epoch 365/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.1049e-05 - val_loss: 2.3809e-05\n",
      "Epoch 366/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.1181e-05 - val_loss: 2.2565e-05\n",
      "Epoch 367/380\n",
      "631/631 [==============================] - 1457s 2s/step - loss: 2.1087e-05 - val_loss: 2.4872e-05\n",
      "Epoch 368/380\n",
      "631/631 [==============================] - 1460s 2s/step - loss: 2.0953e-05 - val_loss: 2.7539e-05\n",
      "Epoch 369/380\n",
      "631/631 [==============================] - 1456s 2s/step - loss: 2.1359e-05 - val_loss: 2.0788e-05\n",
      "Epoch 370/380\n",
      "631/631 [==============================] - 1455s 2s/step - loss: 2.1142e-05 - val_loss: 2.8180e-05\n",
      "Epoch 371/380\n",
      "631/631 [==============================] - 1453s 2s/step - loss: 2.0750e-05 - val_loss: 2.2321e-05\n",
      "Epoch 372/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.1162e-05 - val_loss: 2.8721e-05\n",
      "Epoch 373/380\n",
      "631/631 [==============================] - 1458s 2s/step - loss: 2.0806e-05 - val_loss: 2.2803e-05\n",
      "Epoch 374/380\n",
      "631/631 [==============================] - 1461s 2s/step - loss: 2.0774e-05 - val_loss: 2.4970e-05\n",
      "Epoch 375/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.1093e-05 - val_loss: 3.6830e-05\n",
      "Epoch 376/380\n",
      "631/631 [==============================] - 1451s 2s/step - loss: 2.1180e-05 - val_loss: 2.4599e-05\n",
      "Epoch 377/380\n",
      "631/631 [==============================] - 1446s 2s/step - loss: 2.1229e-05 - val_loss: 2.4237e-05\n",
      "Epoch 378/380\n",
      "631/631 [==============================] - 1447s 2s/step - loss: 2.1149e-05 - val_loss: 2.8522e-05\n",
      "Epoch 379/380\n",
      "631/631 [==============================] - 1448s 2s/step - loss: 2.0847e-05 - val_loss: 2.5234e-05\n",
      "Epoch 380/380\n",
      "631/631 [==============================] - 1442s 2s/step - loss: 2.0712e-05 - val_loss: 1.9689e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(<keras.callbacks.History at 0x7fa94ce4a580>,\n",
       " 'logs/CNN-Model-23_12_2021__20:58:19')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eegcnn.fit(epochs=380, patience=70, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegcnn.save_nn('../cnn_model_2.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eegcnn.load_nn('/media/thanos/Elements/thanos/nn_trained/cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_topos_eval = mat73.loadmat('/media/thanos/Elements/thanos/sim_data/sim_type_1/eval_100/eeg_topos_eval_100.mat')['eeg_topos']\n",
    "eeg_topos_eval = eeg_topos_eval.transpose(2, 0, 1)\n",
    "\n",
    "sources_eval = np.load('/media/thanos/Elements/thanos/sim_data/sim_type_1/eval_100/sources.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_sources = eegcnn.predict(eeg=eeg_topos_eval).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('../../Downloads/predicted_sources.npy',predicted_sources)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e55ae3f6af515937c0319b16e610f5fb778062060e5992134dbdbaaf9a7f178e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
